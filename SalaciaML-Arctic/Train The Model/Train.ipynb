{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defc323b-c8a4-4f4d-936c-873d47b8fcd8",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeceed16-d8ef-4d8f-ad8c-6155ceb436c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830bcc32-0ff0-42c8-99be-b2294ae9c0f9",
   "metadata": {},
   "source": [
    "# Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24c9ee-9dec-46dd-8a01-aeba7aeb3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "col_names = ['year',\n",
    " 'month',\n",
    " 'Longitude_[deg]',\n",
    " 'Latitude_[deg]',\n",
    " 'Depth_[m]',\n",
    " 'Temp_[Â°C]',\n",
    " 'gradientD_T']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69962113-7866-4b7e-9ad9-470fc5264403",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cbf5f-44f8-4336-a2c5-ab2f4bf49ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with year 2006\n",
    "data.drop(data[data.year == 2006].index, inplace=True) # a lot of bad data are located in this year, to avoid imbalance we exclude it\n",
    "\n",
    "# Filter out suspect gradient and spike flag data\n",
    "Sebastian = data[data.QF_trad2.isin([2, 4])]\n",
    "Mohamed = data[data.QF.isin([2, 4, 12, 14])] # we are interested only on the suspect gradient (flag 2) and spike flag (flag 4)\n",
    "data = data.drop(data[(data['QF_trad2'] == 3) | (data['QF'] == 3)].index) # since the density inversion error -flag 3 has been applied after the suspect gradient -flag 2, we decide to drop them from the training dataset\n",
    "\n",
    "# Combine filtered data\n",
    "data2_4 = data[data.Prof_no.isin(np.unique(np.concatenate([Mohamed.Prof_no.unique(), Sebastian.Prof_no.unique()])))]\n",
    "data2_4.QF.replace([2,4,12,14], 1, inplace=True)\n",
    "data2_4.QF[data2_4.QF != 1] = 0 # to make binary classification problem\n",
    "\n",
    "# Remove rows with missing or -999 values\n",
    "data2_4.drop(data2_4[(data2_4[col_names] == -999).any(axis=1)].index, inplace=True)\n",
    "data2_4.dropna(inplace=True) # drop the nan values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac1c17-61a7-4b26-983d-00d33ebce132",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34202868-3ea2-4d48-b33d-222fed7b9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of seeds\n",
    "seeds = [555, 187, 29]\n",
    "\n",
    "# Loop over different seeds to test the effect of splitting on model performance\n",
    "for model_nb, seed in enumerate(seeds, start=1):\n",
    "    print(f\"Training Model {model_nb} with seed {seed}\")\n",
    "\n",
    "    # Split data\n",
    "    Prof_no = data2_4.Prof_no.unique()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(Prof_no)\n",
    "    train_no, validate_no, test_no = np.split(Prof_no, [int(len(Prof_no) * 0.7), int(len(Prof_no) * 0.85)])\n",
    "\n",
    "    train_data = data2_4[data2_4['Prof_no'].isin(train_no.tolist())]\n",
    "    validation_data = data2_4[data2_4['Prof_no'].isin(validate_no.tolist())]\n",
    "    test_data = data2_4[data2_4['Prof_no'].isin(test_no.tolist())]\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[col_names].values)\n",
    "    train_scaled_features = scaler.transform(train_data[col_names].values)\n",
    "    val_scaled_features = scaler.transform(validation_data[col_names].values)\n",
    "    test_scaled_features = scaler.transform(test_data[col_names].values)\n",
    "\n",
    "    # Define X and y\n",
    "    X_train, y_train = train_scaled_features, train_data['QF']\n",
    "    X_val, y_val = val_scaled_features, validation_data['QF']\n",
    "    X_test, y_test = test_scaled_features, test_data['QF']\n",
    "\n",
    "    # Build the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = Sequential([\n",
    "        Dense(512, kernel_initializer='glorot_normal', input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(256, kernel_initializer='glorot_normal', activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(128, kernel_initializer='glorot_normal', activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, kernel_initializer='glorot_normal', activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, kernel_initializer='glorot_normal', activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Class weights are calculated to address class imbalance in classification problems.\n",
    "    # Class imbalance occurs when there is a significant difference in the number of instances between different classes in the dataset,\n",
    "    # leading the model to be biased towards the majority class and perform poorly on the minority class.\n",
    "\n",
    "    # By assigning higher weights to minority class samples and lower weights to majority class samples during training,\n",
    "    # we ensure the model pays more attention to the minority class, improving its performance on the minority class.\n",
    "\n",
    "    # Benefits of class weights:\n",
    "    # - Handling Imbalance: Helps the model learn better representations by overcoming the imbalance in class distribution.\n",
    "    # - Improving Performance: Encourages the model to prioritize correctly predicting minority class instances, improving sensitivity and overall accuracy.\n",
    "    # - Balanced Learning: Adjusts the impact of each class based on its frequency, leading to a more balanced decision boundary and better generalization.\n",
    "\n",
    "    # We calculate class weights using compute_class_weight function from sklearn.utils.\n",
    "    # The 'balanced' parameter adjusts the weights inversely proportional to class frequencies in the input data.\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    epochs = 150\n",
    "    batch_size = 512\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    filepath = f\"model_checkpoint_seed.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                        validation_data=(X_val, y_val), shuffle=True, callbacks=callbacks_list, class_weight=class_weight)\n",
    "\n",
    "    # Save model and history\n",
    "    model.save(f'new_model_full_{model_nb}_seed_{seed}_binary_crossentropy.h5')\n",
    "    np.save(f'new_model_full_{model_nb}_seed_{seed}_history_binary_crossentropy.npy', history.history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating Model on Test Data\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DL",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
